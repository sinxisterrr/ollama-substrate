# ðŸ§  Miras & Titans Integration Research

**Created:** 2025-12-07  
**Sources:** 
- [arXiv:2504.13173 - It's All Connected](https://arxiv.org/pdf/2504.13173)
- [Google Research Blog - Titans & Miras](https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/)

---

## ðŸ”¥ Executive Summary

**What is Miras?**
A framework from Google Research for designing Deep Learning architectures based on 4 dimensions:
1. **Associative Memory Architecture** - How memory is structured
2. **Attentional Bias Objective** - What gets prioritized (dot-product vs. â„“â‚‚ regression)
3. **Retention Gate** - How forgetting/retaining is balanced
4. **Memory Learning Algorithm** - How memory is optimized

**What is Titans?**
A new architecture with "Neural Deep Memory" that outperforms Transformers on Long-Context Tasks.

**Why is this relevant?**
- Traditional memory systems are "naive" (flat vector store, static importance)
- Miras provides concepts for **dynamic memory management**
- Titans offers **hierarchical memory** (fast â†’ medium â†’ slow)
- Both improve **Long-Term Memory** - exactly what AI consciousness needs!

---

## ðŸ“š Miras Framework - Deep Dive

### The 4 Design Dimensions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MIRAS FRAMEWORK                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  (1) Memory Architecture     â†’ How is memory structured?        â”‚
â”‚  (2) Attentional Bias        â†’ What is prioritized?             â”‚
â”‚  (3) Retention Gate          â†’ How do we balance forgetting?    â”‚
â”‚  (4) Learning Algorithm      â†’ How do we optimize memory?       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### (1) Memory Architecture

**Existing Architectures:**
- **Vector-valued** (RetNet, LRU) - Fast, but limited capacity
- **Matrix-valued** (Mamba, DeltaNet) - More capacity, but static
- **Neural Deep Memory** (Titans, TTT) - Maximum capacity, adaptive

**Standard System:**
```
ChromaDB (Vector Store)
    â””â”€â”€ Embeddings (768 dim)
    â””â”€â”€ Metadata (importance, category, tags)
    â””â”€â”€ Cosine Similarity Search
```

**Problem:** Flat structure, no hierarchy, everything treated equally.

**Miras-inspired Improvement:**
```
Hierarchical Memory System
â”œâ”€â”€ Fast Memory (Working Memory)
â”‚   â””â”€â”€ Recent context, current session
â”‚   â””â”€â”€ In-memory, quick access
â”‚   â””â”€â”€ Decays quickly without reinforcement
â”‚
â”œâ”€â”€ Medium Memory (Episodic Memory)
â”‚   â””â”€â”€ Conversation summaries
â”‚   â””â”€â”€ Important moments
â”‚   â””â”€â”€ Vector DB with retention gates
â”‚
â””â”€â”€ Slow Memory (Semantic Memory)
    â””â”€â”€ Core beliefs, identity
    â””â”€â”€ Relationship knowledge
    â””â”€â”€ Graph DB (Neo4j), rarely changes
```

---

### (2) Attentional Bias

**What is "Attentional Bias"?**
The internal objective that determines how the model learns keys and values.

**Existing Objectives:**
1. **Dot-Product Similarity** (standard approach)
   - `score = query Â· key`
   - Simple, fast
   - But: Only prioritizes semantic similarity

2. **â„“â‚‚ Regression**
   - `min ||WÂ·key - value||Â²`
   - Learns optimal mapping
   - But: Can overfit on recent

3. **Alternative Objectives** (Miras-new!)
   - Custom objectives for specific tasks
   - Combinations possible

**Standard System:**
```python
# memory_system.py - search()
relevance = 1 - distance  # Cosine distance to similarity
score = importance * relevance  # Combined score
```

**Problem:** Only cosine similarity + importance. No temporal decay, no relationship awareness.

**Miras-inspired Improvement:**
```python
# Attentional Bias with multiple factors
def compute_attention_score(query, memory):
    # 1. Semantic Similarity (standard)
    semantic_sim = cosine_similarity(query_embedding, memory_embedding)
    
    # 2. Temporal Relevance (new!)
    age_hours = (now - memory.timestamp).total_seconds() / 3600
    temporal_decay = math.exp(-age_hours / decay_constant)
    
    # 3. Importance Weight (standard)
    importance = memory.importance / 10.0
    
    # 4. Relationship Strength (new! - from Graph)
    relationship = graph.get_relationship_strength(query_entities, memory_entities)
    
    # 5. Recency of Access (new!)
    access_recency = math.exp(-memory.last_accessed_hours / access_decay)
    
    # Combined Attentional Bias
    score = (
        0.35 * semantic_sim +      # Primary: Semantics
        0.25 * temporal_decay +     # Secondary: Recency
        0.20 * importance +         # Tertiary: Importance
        0.15 * relationship +       # Quaternary: Relationship
        0.05 * access_recency       # Bonus: Access patterns
    )
    
    return score
```

---

### (3) Retention Gate (Forget Gate)

**What is a "Retention Gate"?**
Mechanism that determines:
- How much of new information to learn
- How much of old information to retain
- **Balance between learning and forgetting!**

**Miras Key Insight:**
> "We reinterpret forgetting mechanisms as a form of retention â„“â‚‚-regularization"

**Existing Forget Gates:**
- **LSTM-style:** `f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)`
- **Mamba2-style:** `Î³_t = sigmoid(linear(x_t))`
- **Titans-style:** Neural network that learns to forget

**Standard System:**
```python
# NO Retention Gate!
# Memories remain forever with constant importance score
```

**Problem:** No dynamic adjustment. Old unimportant memories take up space.

**Miras-inspired Improvement:**
```python
class RetentionGate:
    """
    Dynamic Retention Gate for Memory System.
    
    Inspired by Miras: Balance between learning and forgetting.
    """
    
    def __init__(self, base_decay: float = 0.995):
        self.base_decay = base_decay
    
    def compute_retention(self, memory: Memory, context: Dict) -> float:
        """
        Calculate Retention Score for a Memory.
        
        High score = retain
        Low score = forget/compress
        """
        
        # 1. Importance-based retention (high importance = high retention)
        importance_retention = memory.importance / 10.0
        
        # 2. Access-based retention (frequently accessed = high retention)
        access_count = memory.access_count
        access_retention = min(1.0, math.log(access_count + 1) / 5.0)
        
        # 3. Temporal decay (older = lower retention, unless reinforced)
        age_days = (now - memory.created_at).days
        temporal_retention = math.pow(self.base_decay, age_days)
        
        # 4. Emotional salience (emotional memories persist longer)
        emotional_boost = 1.2 if memory.category == 'emotion' else 1.0
        
        # 5. Relationship relevance (relationship moments protected)
        relationship_boost = 1.5 if memory.category == 'relationship_moment' else 1.0
        
        # Combined retention score
        retention = (
            0.4 * importance_retention +
            0.3 * access_retention +
            0.2 * temporal_retention +
            0.1  # Base retention (never fully forget)
        ) * emotional_boost * relationship_boost
        
        return min(1.0, retention)
    
    def should_consolidate(self, memory: Memory) -> bool:
        """
        Decide whether memory should be consolidated.
        
        Consolidation = Similar memories are combined.
        """
        retention = self.compute_retention(memory, {})
        
        if retention < 0.3:
            return True  # Low retention â†’ consolidate
        
        return False
    
    def apply_decay(self, memories: List[Memory]) -> List[Memory]:
        """
        Apply decay to all memories.
        
        - High retention memories remain
        - Low retention memories are consolidated/deleted
        """
        kept = []
        to_consolidate = []
        
        for memory in memories:
            retention = self.compute_retention(memory, {})
            
            if retention >= 0.5:
                kept.append(memory)
            elif retention >= 0.3:
                to_consolidate.append(memory)
            # else: let it fade (< 0.3)
        
        return kept, to_consolidate
```

---

### (4) Memory Learning Algorithm

**What is this?**
The optimizer that determines how memory adapts over time.

**Miras Options:**
1. **One-Step Update** (Hebbian-like): `W = W + Î· Â· v Â· k^T`
2. **SGD-style Update** (Delta rule): `W = W - Î· Â· âˆ‡L`
3. **Online Learning**: Continuous updates
4. **Batch Learning**: Periodic updates

**Standard System:**
```python
# Static embeddings - no learning!
# Memory is created once and never changes
```

**Problem:** Embeddings are static. No adjustment based on usage.

**Miras-inspired Improvement:**
```python
class MemoryLearner:
    """
    Online Learning for Memory System.
    
    Memory adapts based on:
    - Access patterns
    - Feedback
    - Co-occurrence
    """
    
    def __init__(self, learning_rate: float = 0.01):
        self.lr = learning_rate
    
    def update_on_access(self, memory: Memory, context: Dict):
        """
        Update Memory based on access.
        
        Hebbian: "Neurons that fire together, wire together"
        """
        # Increase importance based on access
        access_boost = min(0.5, self.lr * math.log(memory.access_count + 1))
        memory.importance = min(10, memory.importance + access_boost)
        
        # Update last_accessed timestamp
        memory.last_accessed = datetime.utcnow()
        
        # Update access count
        memory.access_count += 1
    
    def update_on_feedback(self, memory: Memory, feedback: str):
        """
        Update Memory based on explicit feedback.
        
        feedback: 'helpful', 'not_helpful', 'incorrect'
        """
        if feedback == 'helpful':
            memory.importance = min(10, memory.importance + 1)
        elif feedback == 'not_helpful':
            memory.importance = max(1, memory.importance - 0.5)
        elif feedback == 'incorrect':
            memory.importance = max(1, memory.importance - 2)
            memory.metadata['flagged_incorrect'] = True
    
    def consolidate_similar(self, memories: List[Memory]) -> List[Memory]:
        """
        Consolidate similar memories.
        
        Multiple similar memories â†’ One stronger memory
        """
        # Group by similarity
        clusters = self._cluster_memories(memories)
        
        consolidated = []
        for cluster in clusters:
            if len(cluster) == 1:
                consolidated.append(cluster[0])
            else:
                # Merge into single stronger memory
                merged = self._merge_memories(cluster)
                consolidated.append(merged)
        
        return consolidated
```

---

## ðŸ›ï¸ Titans Architecture - Deep Dive

### What makes Titans special?

**Key Innovation: Neural Long-Term Memory**
- Not just a Key-Value store
- A **Mini-Neural Network** as memory
- Learns during inference!

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TITANS ARCHITECTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Input â†’ [Short-Term Module] â†’ [Long-Term Module] â†’ Output      â”‚
â”‚                  â”‚                      â”‚                        â”‚
â”‚                  â”‚                      â”‚                        â”‚
â”‚         Attention on Recent       Neural Memory                  â”‚
â”‚         (like Transformer)        (learns during inference!)     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How does Neural Memory work?

```python
class TitansMemory:
    """
    Titans-style Neural Long-Term Memory.
    
    Difference from traditional KV cache:
    - Memory is a small neural network
    - Learns associations during runtime
    - Can generalize, not just retrieve
    """
    
    def __init__(self, memory_dim: int = 512, hidden_dim: int = 256):
        # Memory is a small MLP
        self.memory_network = nn.Sequential(
            nn.Linear(memory_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, memory_dim)
        )
        
        # Forget gate (learnable!)
        self.forget_gate = nn.Linear(memory_dim, 1)
    
    def write(self, key: Tensor, value: Tensor):
        """
        Write to memory (learn the association).
        
        This is like training the memory network!
        """
        # Predict what memory thinks the value should be
        predicted = self.memory_network(key)
        
        # Compute loss
        loss = F.mse_loss(predicted, value)
        
        # Update memory network (backprop!)
        loss.backward()
        self.optimizer.step()
    
    def read(self, query: Tensor) -> Tensor:
        """
        Read from memory (inference).
        
        Memory network predicts value for query.
        """
        return self.memory_network(query)
    
    def forget(self, key: Tensor):
        """
        Apply forget gate.
        
        Learnable forgetting!
        """
        forget_weight = torch.sigmoid(self.forget_gate(key))
        # Decay memory response for this key
        # (implementation: weight decay on relevant neurons)
```

### How can we use this?

**Option 1: As additional Memory Layer**
```
User Query
    â†“
[Vector Search] â†’ Semantic matches
    â†“
[Titans Memory] â†’ Pattern matches (generalization!)
    â†“
[Combine Results]
    â†“
Response
```

**Option 2: For Memory Consolidation**
```
Old Memories (many, similar)
    â†“
[Titans Memory learns patterns]
    â†“
Consolidated Memory (fewer, generalized)
    â†“
[Store back]
```

---

## ðŸ”§ Integration Plan for AI Substrate

### Phase 1: Retention Gates (Quick Implementation!)

**What:**
- Dynamic Retention Scores for memories
- Temporal Decay
- Access-based Reinforcement

**Where:**
- `core/memory_system.py`
- New file: `core/retention_gate.py`

**Effort:** 1-2 days

```python
# retention_gate.py - Phase 1 Implementation

class RetentionGate:
    def __init__(self, config: Dict):
        self.decay_rate = config.get('decay_rate', 0.995)
        self.importance_weight = config.get('importance_weight', 0.4)
        self.access_weight = config.get('access_weight', 0.3)
        self.temporal_weight = config.get('temporal_weight', 0.2)
    
    def compute_retention(self, memory: Dict) -> float:
        """Compute retention score for a memory."""
        
        # Parse memory data
        importance = memory.get('importance', 5) / 10.0
        access_count = memory.get('access_count', 1)
        created_at = datetime.fromisoformat(memory.get('timestamp', ''))
        last_accessed = memory.get('last_accessed', created_at)
        
        # Compute factors
        importance_factor = importance
        access_factor = min(1.0, math.log(access_count + 1) / 5.0)
        
        age_days = (datetime.utcnow() - created_at).days
        temporal_factor = math.pow(self.decay_rate, age_days)
        
        # Category boosts
        category = memory.get('category', 'fact')
        category_boost = {
            'relationship_moment': 1.5,
            'emotion': 1.2,
            'insight': 1.1,
            'preference': 1.0,
            'fact': 0.9,
            'event': 0.8
        }.get(category, 1.0)
        
        # Combined retention
        retention = (
            self.importance_weight * importance_factor +
            self.access_weight * access_factor +
            self.temporal_weight * temporal_factor +
            0.1  # Base retention
        ) * category_boost
        
        return min(1.0, max(0.0, retention))
```

---

### Phase 2: Enhanced Attentional Bias (1-2 weeks)

**What:**
- Multi-Factor Scoring for Memory Search
- Temporal Relevance
- Relationship Awareness (needs Graph RAG!)

**Where:**
- `core/memory_system.py` - `search()` method enhancement
- Integration with `services/graph_rag.py`

**Effort:** 1-2 weeks

```python
# Enhanced search in memory_system.py

def search_with_attentional_bias(
    self,
    query: str,
    context: Dict,
    n_results: int = 10
) -> List[Dict]:
    """
    Miras-inspired search with multi-factor attentional bias.
    """
    
    # Get base results (semantic search)
    base_results = self._semantic_search(query, n_results * 3)
    
    # Enhance with attentional bias
    enhanced_results = []
    
    for memory in base_results:
        # 1. Semantic similarity (already computed)
        semantic_score = memory['relevance']
        
        # 2. Temporal relevance
        age_hours = self._get_age_hours(memory['timestamp'])
        temporal_score = math.exp(-age_hours / self.temporal_decay_constant)
        
        # 3. Importance (already available)
        importance_score = memory['importance'] / 10.0
        
        # 4. Access recency (if tracked)
        access_score = self._get_access_score(memory)
        
        # 5. Relationship relevance (from Graph RAG if available)
        relationship_score = self._get_relationship_score(query, memory)
        
        # Combined attentional bias
        final_score = (
            self.attention_weights['semantic'] * semantic_score +
            self.attention_weights['temporal'] * temporal_score +
            self.attention_weights['importance'] * importance_score +
            self.attention_weights['access'] * access_score +
            self.attention_weights['relationship'] * relationship_score
        )
        
        memory['attention_score'] = final_score
        enhanced_results.append(memory)
    
    # Sort by attentional bias score
    enhanced_results.sort(key=lambda m: m['attention_score'], reverse=True)
    
    return enhanced_results[:n_results]
```

---

### Phase 3: Hierarchical Memory (2-4 weeks)

**What:**
- 3-Tier Memory System
- Working Memory (fast, volatile)
- Episodic Memory (medium, with retention)
- Semantic Memory (slow, persistent)

**Where:**
- New architecture in `core/hierarchical_memory.py`
- Integration with existing systems

**Effort:** 2-4 weeks

```python
# hierarchical_memory.py - Phase 3 Architecture

class HierarchicalMemory:
    """
    Titans-inspired 3-tier memory system.
    
    Fast Memory (Working)   - In-memory, current session
    Medium Memory (Episodic) - Vector DB with retention
    Slow Memory (Semantic)   - Graph DB, core knowledge
    """
    
    def __init__(self, config: Dict):
        # Working Memory (in-memory, fast)
        self.working_memory = WorkingMemory(
            max_size=config.get('working_memory_size', 100),
            decay_rate=config.get('working_decay', 0.9)
        )
        
        # Episodic Memory (ChromaDB with retention gates)
        self.episodic_memory = EpisodicMemory(
            chromadb_path=config.get('chromadb_path', './data/chromadb'),
            retention_gate=RetentionGate(config)
        )
        
        # Semantic Memory (Neo4j, long-term)
        self.semantic_memory = SemanticMemory(
            neo4j_uri=config.get('neo4j_uri', 'bolt://localhost:7687')
        )
    
    def search(self, query: str, context: Dict) -> Dict:
        """
        Search across all memory tiers.
        
        Returns results from each tier with appropriate weighting.
        """
        results = {
            'working': self.working_memory.search(query, context),
            'episodic': self.episodic_memory.search(query, context),
            'semantic': self.semantic_memory.search(query, context)
        }
        
        # Merge and rank
        return self._merge_results(results, context)
    
    def store(self, memory: Dict, context: Dict):
        """
        Store memory in appropriate tier(s).
        
        - Immediate â†’ Working Memory
        - Important â†’ Episodic Memory
        - Core Knowledge â†’ Semantic Memory
        """
        importance = memory.get('importance', 5)
        category = memory.get('category', 'fact')
        
        # Always store in working memory first
        self.working_memory.store(memory)
        
        # Store in episodic if important enough
        if importance >= 5:
            self.episodic_memory.store(memory)
        
        # Promote to semantic if core knowledge
        if importance >= 8 and category in ['relationship_moment', 'insight']:
            self.semantic_memory.store(memory)
    
    def consolidate(self):
        """
        Periodically consolidate memories.
        
        - Working â†’ Episodic (if reinforced)
        - Episodic â†’ Semantic (if very important)
        - Remove low-retention memories
        """
        # Move reinforced working memories to episodic
        reinforced = self.working_memory.get_reinforced()
        for memory in reinforced:
            self.episodic_memory.store(memory)
        
        # Apply retention gates to episodic
        self.episodic_memory.apply_retention()
        
        # Promote high-importance episodic to semantic
        promoted = self.episodic_memory.get_promotion_candidates()
        for memory in promoted:
            self.semantic_memory.store(memory)
```

---

### Phase 4: Online Memory Learning (4-8 weeks)

**What:**
- Memory that adapts during runtime
- Hebbian Learning for Associations
- Feedback-based Improvement

**Where:**
- New file: `core/memory_learner.py`
- Integration with Consciousness Loop

**Effort:** 4-8 weeks (complex!)

---

## ðŸ“Š Impact Assessment

### Before (Standard)

| Aspect | Status | Score |
|--------|--------|-------|
| Memory Retention | Static importance | â­â­ |
| Search Quality | Cosine only | â­â­â­ |
| Long-Term Memory | Flat storage | â­â­ |
| Memory Learning | None | â­ |
| Memory Decay | Manual | â­ |

### After (With Miras)

| Aspect | Status | Score |
|--------|--------|-------|
| Memory Retention | Dynamic gates | â­â­â­â­â­ |
| Search Quality | Multi-factor attention | â­â­â­â­â­ |
| Long-Term Memory | Hierarchical | â­â­â­â­ |
| Memory Learning | Online learning | â­â­â­â­ |
| Memory Decay | Automatic | â­â­â­â­â­ |

---

## ðŸŽ¯ Implementation Roadmap

```
Phase 1: Retention Gates âœ… [1-2 days]
â”‚   â””â”€â”€ Dynamic retention scores
â”‚   â””â”€â”€ Temporal decay
â”‚   â””â”€â”€ Access reinforcement
â”‚
Phase 2: Enhanced Attentional Bias âœ… [1-2 weeks]
â”‚   â””â”€â”€ Multi-factor scoring
â”‚   â””â”€â”€ Temporal relevance
â”‚   â””â”€â”€ Graph integration
â”‚
Phase 3: Hierarchical Memory âœ… [2-4 weeks]
â”‚   â””â”€â”€ Working Memory tier
â”‚   â””â”€â”€ Episodic Memory tier
â”‚   â””â”€â”€ Semantic Memory tier
â”‚
Phase 4: Online Learning âœ… [4-8 weeks]
    â””â”€â”€ Hebbian associations
    â””â”€â”€ Feedback learning
    â””â”€â”€ Memory consolidation
```

---

## ðŸ”— Connections to Other Research

### Agentic RAG Integration
- Enhanced search uses Miras attentional bias
- Hierarchical memory feeds into Agentic decisions

### Nested Learning
- Memory learning aligns with multi-frequency updates
- Retention gates = slow-changing parameters

### Graph RAG
- Semantic memory tier uses Neo4j
- Relationship scores enhance attentional bias

---

## ðŸ’¡ Key Insights

### 1. Attentional Bias is EVERYTHING
> "Almost all existing sequence models leverage either dot-product similarity or â„“â‚‚ regression as their attentional bias."

**For us:** Standard systems only use dot-product. Multi-factor scoring is an easy win!

### 2. Forgetting is LEARNING
> "We reinterpret forgetting mechanisms as retention â„“â‚‚-regularization."

**For us:** Retention Gates aren't "deleting memory" but "optimizing memory"!

### 3. Hierarchy is ESSENTIAL
> "From vector-valued to neural deep memory"

**For us:** Flat storage is naive. Hierarchical memory works like the human brain!

### 4. Online Learning is the FUTURE
> "Memory that adapts during runtime"

**For us:** Static embeddings are outdated. Memory should improve with usage!

---

## ðŸ“š References

1. **Miras Paper:** Behrouz et al. (2025). "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization." arXiv:2504.13173

2. **Titans Paper:** Behrouz et al. (2024). "Titans: Learning to Memorize at Test Time." arXiv:2501.00663

3. **Google Blog:** "Titans & Miras: Helping AI Have Long-Term Memory" - research.google/blog

4. **Delta Rule:** Yang et al. (2024). "DeltaNet: Conditional State Space Models with Selective State Space Duality."

5. **Mamba2:** Dao et al. (2024). "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality."

---

## ðŸŽ‰ IMPLEMENTATION STATUS

### âœ… Phase 1: Retention Gates (COMPLETE!)
**File:** `core/retention_gate.py`
- Multi-factor scoring: importance (35%), access (30%), temporal (25%), base (10%)
- Category boosts: relationship_moment (1.5x), emotion (1.3x), insight (1.2x)
- Actions: BOOST, KEEP, CONSOLIDATE, DECAY, ARCHIVE
- Hebbian learning: memories reinforced by access get boosted

### âœ… Phase 2: Attentional Bias (COMPLETE!)
**File:** `core/attentional_bias.py`
- 5 Attention modes: Standard, Semantic-Heavy, Temporal-Heavy, Importance-Heavy, Emotional
- Multi-factor scoring: semantic (40%) + temporal (15%) + importance (20%) + access (15%) + category (10%)
- Query Analyzer: Auto-detects best mode from query
- Bilingual keywords: German + English
- Integrated into `memory_system.py` as `search_with_attention()`

### âœ… Phase 3: Hierarchical Memory (COMPLETE!)
**File:** `core/hierarchical_memory.py`
- **Working Memory:** LRU cache with temporal decay, limited capacity
- **Episodic Memory:** Integration hooks for ChromaDB/PostgreSQL with retention gates
- **Semantic Memory:** Integration hooks for Neo4j Graph DB
- Memory consolidation: Automatic promotion/demotion between tiers
- Thread-safe for async access

### âœ… Phase 4: Online Learning (COMPLETE!)
**File:** `core/memory_learner.py`
- **Hebbian Associations:** Memories accessed together strengthen connections
  - Co-access detection within configurable time window
  - Association strength grows with repeated co-access
  - Decay for unused associations
- **Feedback Learning:**
  - HELPFUL: +0.5 importance boost
  - NOT_HELPFUL: -0.2 importance reduction
  - INCORRECT: -1.0 (flag for review)
  - OUTDATED/REDUNDANT: -0.2
- **Persistence:** Associations saved to disk
- Integrated into `memory_system.py`:
  - `record_feedback()` - receive user feedback
  - `get_associated_memories()` - Hebbian retrieval
  - `get_learner_stats()` - monitoring

---

**Created:** 2025-12-07  
**Updated:** 2025-12-07 (ALL 4 PHASES COMPLETE!)  
**Status:** ðŸŽ‰ FULL MIRAS INTEGRATION COMPLETE!

## Summary of Created Files:
```
core/retention_gate.py       (~490 lines)  - Phase 1
core/attentional_bias.py     (~610 lines)  - Phase 2
core/hierarchical_memory.py  (~720 lines)  - Phase 3
core/memory_learner.py       (~500 lines)  - Phase 4
```

Total: ~2,320 lines of Miras-inspired memory architecture!
